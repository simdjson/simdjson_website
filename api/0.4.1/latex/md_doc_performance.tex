simdjson strives to be at its fastest {\itshape without tuning}, and generally achieves this. However, there are still some scenarios where tuning can enhance performance.


\begin{DoxyItemize}
\item \href{\#reusing-the-parser-for-maximum-efficiency}{\texttt{ Reusing the parser for maximum efficiency}}
\begin{DoxyItemize}
\item \href{\#keeping-documents-around-for-longer}{\texttt{ Keeping documents around for longer}}
\end{DoxyItemize}
\item \href{\#server-loops-long-running-processes-and-memory-capacity}{\texttt{ Server Loops\+: Long-\/\+Running Processes and Memory Capacity}}
\item \href{\#large-files-and-huge-page-support}{\texttt{ Large files and huge page support}}
\item \href{\#computed-gotos}{\texttt{ Computed G\+O\+T\+Os}}
\item \href{\#number-parsing}{\texttt{ Number parsing}}
\item \href{\#visual-studio}{\texttt{ Visual Studio}}
\item \href{\#downclocking}{\texttt{ Downclocking}}
\end{DoxyItemize}\hypertarget{md_doc_performance_autotoc_md49}{}\doxysection{Reusing the parser for maximum efficiency}\label{md_doc_performance_autotoc_md49}
If you\textquotesingle{}re using simdjson to parse multiple documents, or in a loop, you should make a parser once and reuse it. The simdjson library will allocate and retain internal buffers between parses, keeping buffers hot in cache and keeping memory allocation and initialization to a minimum. In this manner, you can parse terabytes of J\+S\+ON data without doing any new allocation.


\begin{DoxyCode}{0}
\DoxyCodeLine{ \{c++\}}
\DoxyCodeLine{dom::parser parser;}
\DoxyCodeLine{}
\DoxyCodeLine{// This initializes buffers and a document big enough to handle this JSON.}
\DoxyCodeLine{dom::element doc = parser.parse("[ true, false ]"\_padded);}
\DoxyCodeLine{cout << doc << endl;}
\DoxyCodeLine{}
\DoxyCodeLine{// This reuses the existing buffers, and reuses and *overwrites* the old document}
\DoxyCodeLine{doc = parser.parse("[1, 2, 3]"\_padded);}
\DoxyCodeLine{cout << doc << endl;}
\DoxyCodeLine{}
\DoxyCodeLine{// This also reuses the existing buffers, and reuses and *overwrites* the old document}
\DoxyCodeLine{dom::element doc2 = parser.parse("true"\_padded);}
\DoxyCodeLine{// Even if you keep the old reference around, doc and doc2 refer to the same document.}
\DoxyCodeLine{cout << doc << endl;}
\DoxyCodeLine{cout << doc2 << endl;}
\end{DoxyCode}


It\textquotesingle{}s not just internal buffers though. The simdjson library reuses the document itself. The dom\+::element, dom\+::object and dom\+::array instances are {\itshape references} to the internal document. You are only {\itshape borrowing} the document from simdjson, which purposely reuses and overwrites it each time you call parse. This prevent wasteful and unnecessary memory allocation in 99\% of cases where J\+S\+ON is just read, used, and converted to native values or thrown away.

\begin{quote}
{\bfseries{You are only borrowing the document from the simdjson parser. Don\textquotesingle{}t keep it long term!}} \end{quote}


This is key\+: don\textquotesingle{}t keep the {\ttfamily document\&}, {\ttfamily dom\+::element}, {\ttfamily dom\+::array}, {\ttfamily dom\+::object} or {\ttfamily string\+\_\+view} objects you get back from the A\+PI. Convert them to C++ native values, structs and arrays that you own.\hypertarget{md_doc_performance_autotoc_md50}{}\doxysection{Server Loops\+: Long-\/\+Running Processes and Memory Capacity}\label{md_doc_performance_autotoc_md50}
The simdjson library automatically expands its memory capacity when larger documents are parsed, so that you don\textquotesingle{}t unexpectedly fail. In a short process that reads a bunch of files and then exits, this works pretty flawlessly.

Server loops, though, are long-\/running processes that will keep the parser around forever. This means that if you encounter a really, really large document, simdjson will not resize back down. The simdjson library lets you adjust your allocation strategy to prevent your server from growing without bound\+:


\begin{DoxyItemize}
\item You can set a {\itshape max capacity} when constructing a parser\+:
\end{DoxyItemize}


\begin{DoxyCode}{0}
\DoxyCodeLine{\{c++\}}
\DoxyCodeLine{ dom::parser parser(1000*1000); // Never grow past documents > 1MB}
\DoxyCodeLine{ for (web\_request request : listen()) \{}
\DoxyCodeLine{   dom::element doc;}
\DoxyCodeLine{   auto error = parser.parse(request.body).get(doc);}
\DoxyCodeLine{   // If the document was above our limit, emit 413 = payload too large}
\DoxyCodeLine{   if (error == CAPACITY) \{ request.respond(413); continue; \}}
\DoxyCodeLine{   // ...}
\DoxyCodeLine{ \}}
\end{DoxyCode}


This parser will grow normally as it encounters larger documents, but will never pass 1MB.


\begin{DoxyItemize}
\item You can set a {\itshape fixed capacity} that never grows, as well, which can be excellent for predictability and reliability, since simdjson will never call malloc after startup!
\end{DoxyItemize}


\begin{DoxyCode}{0}
\DoxyCodeLine{\{c++\}}
\DoxyCodeLine{ dom::parser parser(0); // This parser will refuse to automatically grow capacity}
\DoxyCodeLine{ auto error = parser.allocate(1000*1000); // This allocates enough capacity to handle documents <= 1MB}
\DoxyCodeLine{ if (error) \{ cerr << error << endl; exit(1); \}}
\DoxyCodeLine{}
\DoxyCodeLine{ for (web\_request request : listen()) \{}
\DoxyCodeLine{   dom::element doc;}
\DoxyCodeLine{   error = parser.parse(request.body).get(doc);}
\DoxyCodeLine{   // If the document was above our limit, emit 413 = payload too large}
\DoxyCodeLine{   if (error == CAPACITY) \{ request.respond(413); continue; \}}
\DoxyCodeLine{   // ...}
\DoxyCodeLine{ \}}
\end{DoxyCode}
\hypertarget{md_doc_performance_autotoc_md51}{}\doxysection{Large files and huge page support}\label{md_doc_performance_autotoc_md51}
There is a memory allocation performance cost the first time you process a large file (e.\+g. 100MB). Between the cost of allocation, the fact that the memory is not in cache, and the initial zeroing of memory, \href{https://lemire.me/blog/2020/01/14/how-fast-can-you-allocate-a-large-block-of-memory-in-c/}{\texttt{ on some systems, allocation runs far slower than parsing (e.\+g., 1.\+4\+GB/s)}}. Reusing the parser mitigates this by paying the cost once, but does not eliminate it.

In large file use cases, enabling transparent huge page allocation on the OS can help a lot. We haven\textquotesingle{}t found the right way to do this on Windows or O\+S/X, but on Linux, you can enable transparent huge page allocation with a command like\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{echo always > /sys/kernel/mm/transparent\_hugepage/enabled}
\end{DoxyCode}


In general, when running benchmarks over large files, we recommend that you report performance numbers with and without huge pages if possible. Furthermore, you should amortize the parsing (e.\+g., by parsing several large files) to distinguish the time spent parsing from the time spent allocating memory. If you are using the {\ttfamily parse} benchmarking tool provided with the simdjson library, you can use the {\ttfamily -\/H} flag to omit the memory allocation cost from the benchmark results.


\begin{DoxyCode}{0}
\DoxyCodeLine{./parse largefile \# includes memory allocation cost}
\DoxyCodeLine{./parse -\/H largefile \# without memory allocation}
\end{DoxyCode}
\hypertarget{md_doc_performance_autotoc_md52}{}\doxysection{Computed G\+O\+T\+Os}\label{md_doc_performance_autotoc_md52}
For best performance, we use a technique called \char`\"{}computed goto\char`\"{} when the compiler supports it, it is also sometimes described as \char`\"{}\+Labels as Values\char`\"{}. Though it is not part of the C++ standard, it is supported by many major compilers and it brings measurable performance benefits that are difficult to achieve otherwise. The computed gotos are automatically disabled under Visual Studio.

If you wish to forcefully disable computed gotos, you can do so by compiling the code with {\ttfamily -\/D\+S\+I\+M\+D\+J\+S\+O\+N\+\_\+\+N\+O\+\_\+\+C\+O\+M\+P\+U\+T\+E\+D\+\_\+\+G\+O\+TO=1}. It is not recommended to disable computed gotos if your compiler supports it. In fact, you should almost never need to be concerned with computed gotos.\hypertarget{md_doc_performance_autotoc_md53}{}\doxysection{Number parsing}\label{md_doc_performance_autotoc_md53}
Some J\+S\+ON files contain many floating-\/point values. It is the case with many Geo\+J\+S\+ON files. Accurately parsing decimal strings into binary floating-\/point values with proper rounding is challenging. To our knowledge, it is not possible, in general, to parse streams of numbers at gigabytes per second using a single core. While using the simdjson library, it is possible that you might be limited to a few hundred megabytes per second if your J\+S\+ON documents are densely packed with floating-\/point values.


\begin{DoxyItemize}
\item When possible, you should favor integer values written without a decimal point, as it simpler and faster to parse decimal integer values.
\item When serializing numbers, you should not use more digits than necessary\+: 17 digits is all that is needed to exactly represent double-\/precision floating-\/point numbers. Using many more digits than necessary will make your files larger and slower to parse.
\item When benchmarking parsing speeds, always report whether your J\+S\+ON documents are made mostly of floating-\/point numbers when it is the case, since number parsing can then dominate the parsing time.
\end{DoxyItemize}\hypertarget{md_doc_performance_autotoc_md54}{}\doxysection{Visual Studio}\label{md_doc_performance_autotoc_md54}
On Intel and A\+MD Windows platforms, Microsoft Visual Studio enables programmers to build either 32-\/bit (x86) or 64-\/bit (x64) binaries. We urge you to always use 64-\/bit mode. Visual Studio 2019 should default on 64-\/bit builds when you have a 64-\/bit version of Windows, which we recommend.

We do not recommend that you compile simdjson with architecture-\/specific flags such as {\ttfamily arch\+:A\+V\+X2}. The simdjson library automatically selects the best execution kernel at runtime.

Recent versions of Microsoft Visual Studio on Windows provides support for the L\+L\+VM Clang compiler. You only need to install the \char`\"{}\+Clang compiler\char`\"{} optional component. You may also get a copy of the 64-\/bit L\+L\+VM C\+Lang compiler for \href{https://releases.llvm.org/download.html}{\texttt{ Windows directly from L\+L\+VM}}. The simdjson library fully supports the L\+L\+VM Clang compiler under Windows. In fact, you may get better performance out of simdjson with the L\+L\+VM Clang compiler than with the regular Visual Studio compiler.\hypertarget{md_doc_performance_autotoc_md55}{}\doxysection{Downclocking}\label{md_doc_performance_autotoc_md55}
You should not expect the simdjson library to cause downclocking of your recent Intel C\+PU cores.

On some Intel processors, using S\+I\+MD instructions in a sustained manner on the same C\+PU core may result in a phenomenon called downclocking whereas the processor initially runs these instructions at a slow speed before reducing the frequency of the core for a short time (milliseconds). Intel refers to these states as licenses. On some current Intel processors, it occurs under two scenarios\+:


\begin{DoxyItemize}
\item \href{https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/}{\texttt{ Whenever 512-\/bit A\+V\+X-\/512 instructions are used}}.
\item Whenever heavy 256-\/bit or wider instructions are used. Heavy instructions are those involving floating point operations or integer multiplications (since these execute on the floating point unit).
\end{DoxyItemize}

The simdjson library does not currently support A\+V\+X-\/512 instructions and it does not make use of heavy 256-\/bit instructions. Thus there should be no downclocking due to simdjson on recent processors. You may still be worried about which S\+I\+MD instruction set is used by simdjson. Thankfully, you can always determine and change which architecture-\/specific implementation is used. Thus even if your C\+PU supports A\+V\+X2, you do not need to use A\+V\+X2. You are in control. 